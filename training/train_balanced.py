#!/usr/bin/env python3
"""
YOLOv12 Training Script for Balanced Dataset
Script huáº¥n luyá»‡n YOLOv12 cho dataset balanced vá»›i checkpoint resume
"""

import os
import sys
import json
import time
import logging
from pathlib import Path
from datetime import datetime
import subprocess
import yaml
import signal
import atexit

class BalancedTrainer:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.dataset_path = self.project_root / "datasets" / "traffic_ai_balanced_11class_processed"
        self.data_yaml = self.dataset_path / "data.yaml"
        self.runs_dir = self.project_root / "runs" / "balanced"
        self.model_weights = self.project_root / "yolo12n.pt"
        
        # Training parameters
        self.epochs = 100
        self.batch_size = 16
        self.img_size = 640
        self.patience = 20
        self.device = "0"
        
        # Logging setup
        self.setup_logging()
        self.setup_signal_handlers()
        
    def setup_logging(self):
        """Setup logging to file and console"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_dir = self.runs_dir / f"balanced_training_{timestamp}"
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        log_file = self.log_dir / "training.log"
        
        # Configure logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_signal_handlers(self):
        """Setup signal handlers for graceful shutdown"""
        def signal_handler(signum, frame):
            self.logger.info(f"ğŸ›‘ Received signal {signum}. Gracefully shutting down...")
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
    def check_prerequisites(self):
        """Check if all required files exist"""
        self.logger.info("ğŸ” Checking prerequisites...")
        
        if not self.data_yaml.exists():
            raise FileNotFoundError(f"âŒ Data config not found: {self.data_yaml}")
        
        if not self.model_weights.exists():
            raise FileNotFoundError(f"âŒ Model weights not found: {self.model_weights}")
        
        # Load and validate data config
        with open(self.data_yaml, 'r', encoding='utf-8') as f:
            data_config = yaml.safe_load(f)
        
        train_path = self.dataset_path / data_config['train']
        val_path = self.dataset_path / data_config['val']
        
        if not train_path.exists():
            raise FileNotFoundError(f"âŒ Train images not found: {train_path}")
        if not val_path.exists():
            raise FileNotFoundError(f"âŒ Val images not found: {val_path}")
        
        train_count = len(list(train_path.glob('*.jpg')))
        val_count = len(list(val_path.glob('*.jpg')))
        
        self.logger.info(f"âœ… Dataset validated: {train_count} train, {val_count} val images")
        self.logger.info(f"âœ… Classes: {data_config.get('nc', 0)}")
        
        return data_config
        
    def find_latest_checkpoint(self):
        """Find the latest training checkpoint to resume from"""
        if not self.runs_dir.exists():
            return None
            
        # Look for existing training runs
        existing_runs = [d for d in self.runs_dir.iterdir() if d.is_dir()]
        if not existing_runs:
            return None
            
        # Find the most recent run with weights
        latest_run = None
        latest_time = 0
        
        for run_dir in existing_runs:
            weights_dir = run_dir / "weights"
            if weights_dir.exists():
                last_pt = weights_dir / "last.pt"
                if last_pt.exists():
                    mod_time = last_pt.stat().st_mtime
                    if mod_time > latest_time:
                        latest_time = mod_time
                        latest_run = run_dir
        
        if latest_run:
            checkpoint = latest_run / "weights" / "last.pt"
            self.logger.info(f"ğŸ”„ Found checkpoint to resume: {checkpoint}")
            return checkpoint
        
        return None
        
    def train(self, resume_checkpoint=None):
        """Run YOLOv12 training"""
        self.logger.info("ğŸš€ STARTING BALANCED DATASET TRAINING")
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ“Š Dataset: {self.dataset_path.name}")
        self.logger.info(f"ğŸ”„ Epochs: {self.epochs}")
        self.logger.info(f"ğŸ“¦ Batch size: {self.batch_size}")
        self.logger.info(f"ğŸ“ Image size: {self.img_size}")
        self.logger.info(f"â³ Patience: {self.patience}")
        self.logger.info(f"ğŸ–¥ï¸ Device: {self.device}")
        self.logger.info(f"ğŸ“ Output: {self.log_dir}")
        self.logger.info("=" * 60)
        
        # Build training command
        cmd = [
            'yolo',
            'task=detect',
            'mode=train',
            f'data={self.data_yaml}',
            f'epochs={self.epochs}',
            f'batch={self.batch_size}',
            f'imgsz={self.img_size}',
            f'patience={self.patience}',
            f'project={self.runs_dir}',
            f'name=balanced_training_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
            f'device={self.device}',
            'save=True',
            'save_period=5',  # Save every 5 epochs
            'plots=True',
            'val=True',
            'verbose=True'
        ]
        
        # Add model or resume checkpoint
        if resume_checkpoint:
            cmd.append(f'model={resume_checkpoint}')
            cmd.append('resume=True')
            self.logger.info(f"ğŸ”„ Resuming from: {resume_checkpoint}")
        else:
            cmd.append(f'model={self.model_weights}')
            self.logger.info(f"ğŸ†• Starting fresh training")
        
        self.logger.info(f"ğŸ’» Command: {' '.join(cmd)}")
        
        # Start training
        try:
            # Run with real-time output
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            # Log output in real-time
            for line in process.stdout:
                line = line.strip()
                if line:
                    self.logger.info(line)
                    
                    # Parse and highlight important metrics
                    if 'mAP50' in line or 'mAP50-95' in line:
                        self.logger.info(f"ğŸ“Š METRICS: {line}")
                    elif 'loss' in line.lower():
                        self.logger.info(f"ğŸ“‰ LOSS: {line}")
            
            process.wait()
            
            if process.returncode == 0:
                self.logger.info("ğŸ‰ TRAINING COMPLETED SUCCESSFULLY!")
                return True
            else:
                self.logger.error(f"âŒ Training failed with exit code: {process.returncode}")
                return False
                
        except KeyboardInterrupt:
            self.logger.info("â¹ï¸ Training interrupted by user")
            if 'process' in locals():
                process.terminate()
            return False
        except Exception as e:
            self.logger.error(f"âŒ Training error: {e}")
            return False
    
    def save_training_info(self, data_config):
        """Save training configuration and info"""
        info = {
            "dataset_type": "balanced",
            "dataset_path": str(self.dataset_path),
            "model_weights": str(self.model_weights),
            "epochs": self.epochs,
            "batch_size": self.batch_size,
            "img_size": self.img_size,
            "patience": self.patience,
            "device": self.device,
            "num_classes": data_config.get('nc', 0),
            "class_names": data_config.get('names', []),
            "training_start": datetime.now().isoformat(),
            "log_dir": str(self.log_dir)
        }
        
        info_file = self.log_dir / "training_info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ’¾ Training info saved: {info_file}")
    
    def run(self):
        """Main training pipeline"""
        try:
            # Check prerequisites
            data_config = self.check_prerequisites()
            
            # Save training info
            self.save_training_info(data_config)
            
            # Check for existing checkpoint
            checkpoint = self.find_latest_checkpoint()
            
            # Ask user about resuming
            if checkpoint:
                response = input(f"\nğŸ”„ Found checkpoint: {checkpoint.name}\nResume training? (y/n): ").lower()
                if response not in ['y', 'yes']:
                    checkpoint = None
                    self.logger.info("ğŸ†• Starting fresh training as requested")
            
            # Run training
            success = self.train(checkpoint)
            
            if success:
                self.logger.info("âœ… Balanced dataset training completed!")
                print(f"\nğŸ‰ Training completed successfully!")
                print(f"ğŸ“ Results saved in: {self.log_dir}")
                print(f"ğŸ“Š Check runs/balanced/ for all outputs")
            else:
                self.logger.error("âŒ Training failed!")
                sys.exit(1)
                
        except Exception as e:
            self.logger.error(f"âŒ Fatal error: {e}")
            sys.exit(1)

def main():
    """Main function"""
    print("ğŸš€ YOLOv12 Balanced Dataset Training")
    print("=" * 50)
    
    trainer = BalancedTrainer()
    trainer.run()

if __name__ == "__main__":
    main()