#!/usr/bin/env python3
"""
GPU Memory Check & Optimization for RTX 3050 Ti
Kiá»ƒm tra GPU memory vÃ  Ä‘Æ°a ra khuyáº¿n nghá»‹ tá»‘i Æ°u
"""

import torch
import psutil
import subprocess
import sys
from pathlib import Path

def check_gpu_info():
    """Kiá»ƒm tra thÃ´ng tin GPU"""
    print("ğŸ” GPU Information:")
    print("=" * 50)
    
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"ğŸ“Š Available GPUs: {gpu_count}")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9
            print(f"ğŸ–¥ï¸ GPU {i}: {gpu_name}")
            print(f"ğŸ’¾ Total Memory: {gpu_memory:.1f} GB")
            
            # Check current memory usage
            torch.cuda.empty_cache()
            allocated = torch.cuda.memory_allocated(i) / 1e9
            cached = torch.cuda.memory_reserved(i) / 1e9
            free = gpu_memory - cached
            
            print(f"ğŸ“ˆ Allocated: {allocated:.2f} GB")
            print(f"ğŸ—„ï¸ Cached: {cached:.2f} GB")
            print(f"ğŸ†“ Free: {free:.2f} GB")
            print()
    else:
        print("âŒ CUDA not available")
        return False
    
    return True

def check_system_ram():
    """Kiá»ƒm tra RAM há»‡ thá»‘ng"""
    print("ğŸ” System RAM Information:")
    print("=" * 30)
    
    ram = psutil.virtual_memory()
    total_gb = ram.total / 1e9
    available_gb = ram.available / 1e9
    used_gb = ram.used / 1e9
    
    print(f"ğŸ’¾ Total RAM: {total_gb:.1f} GB")
    print(f"ğŸ†“ Available: {available_gb:.1f} GB")
    print(f"ğŸ“ˆ Used: {used_gb:.1f} GB")
    print(f"ğŸ“Š Usage: {ram.percent:.1f}%")
    print()

def recommend_settings():
    """ÄÆ°a ra khuyáº¿n nghá»‹ settings cho RTX 3050 Ti"""
    print("ğŸ’¡ Recommended Settings for RTX 3050 Ti (4GB VRAM):")
    print("=" * 60)
    
    recommendations = {
        "batch_size": "4 (hoáº·c 8 náº¿u image size nhá» hÆ¡n)",
        "image_size": "640 (cÃ³ thá»ƒ giáº£m xuá»‘ng 512 náº¿u cáº§n)",
        "amp": "True (Mixed Precision Ä‘á»ƒ tiáº¿t kiá»‡m VRAM)",
        "cache": "False (TrÃ¡nh cache Ä‘á»ƒ tiáº¿t kiá»‡m RAM)",
        "workers": "4 (Giáº£m sá»‘ workers)",
        "save_period": "10 (Tiáº¿t kiá»‡m disk space)",
        "patience": "15-20 (Early stopping)",
        "epochs": "50-100 (Tuá»³ dataset size)"
    }
    
    for param, value in recommendations.items():
        print(f"âš™ï¸ {param}: {value}")
    
    print("\nğŸ“‹ Additional Tips:")
    print("â€¢ ÄÃ³ng cÃ¡c á»©ng dá»¥ng khÃ¡c Ä‘á»ƒ giáº£i phÃ³ng VRAM")
    print("â€¢ Sá»­ dá»¥ng CPU training náº¿u GPU quÃ¡ nhá»: device='cpu'")
    print("â€¢ Chia nhá» dataset náº¿u quÃ¡ lá»›n")
    print("â€¢ Monitor GPU usage: nvidia-smi")

def test_memory_usage():
    """Test memory usage vá»›i settings khuyáº¿n nghá»‹"""
    if not torch.cuda.is_available():
        print("âŒ CUDA not available for testing")
        return
    
    print("\nğŸ§ª Testing Memory Usage:")
    print("=" * 30)
    
    device = torch.device('cuda:0')
    
    try:
        # Test vá»›i batch size 4
        batch_size = 4
        img_size = 640
        
        # Simulate YOLOv12 input
        test_input = torch.randn(batch_size, 3, img_size, img_size).to(device)
        
        allocated = torch.cuda.memory_allocated(0) / 1e9
        print(f"âœ… Test successful with batch_size={batch_size}, img_size={img_size}")
        print(f"ğŸ“ˆ Memory used: {allocated:.2f} GB")
        
        # Test vá»›i batch size 8
        try:
            test_input = torch.randn(8, 3, img_size, img_size).to(device)
            allocated = torch.cuda.memory_allocated(0) / 1e9
            print(f"âœ… Test successful with batch_size=8, img_size={img_size}")
            print(f"ğŸ“ˆ Memory used: {allocated:.2f} GB")
        except RuntimeError as e:
            print(f"âŒ batch_size=8 failed: Out of memory")
            print("ğŸ’¡ Khuyáº¿n nghá»‹: Sá»­ dá»¥ng batch_size=4")
        
        torch.cuda.empty_cache()
        
    except RuntimeError as e:
        print(f"âŒ Memory test failed: {e}")
        print("ğŸ’¡ Khuyáº¿n nghá»‹: Giáº£m batch_size hoáº·c image_size")

def update_training_configs():
    """Cáº­p nháº­t config files vá»›i settings tá»‘i Æ°u"""
    print("\nğŸ”§ Updating Training Configurations:")
    print("=" * 40)
    
    project_root = Path(__file__).parent.parent
    
    # ThÃ´ng tin tá»‘i Æ°u cho RTX 3050 Ti
    optimal_config = {
        "batch_size": 4,
        "img_size": 640,
        "amp": True,
        "cache": False,
        "workers": 4,
        "save_period": 10,
        "patience": 20
    }
    
    print("âœ… Optimal configurations:")
    for key, value in optimal_config.items():
        print(f"   {key}: {value}")
    
    print("\nğŸ’¾ Configurations already applied to training scripts!")
    print("ğŸš€ Ready to start training with optimized settings")

def main():
    """Main function"""
    print("ğŸ”§ GPU Memory Optimizer for YOLOv12 Training")
    print("=" * 60)
    print("Hardware: RTX 3050 Ti (4GB VRAM) + 16GB RAM + i5-12500H")
    print("=" * 60)
    
    # Check GPU
    gpu_ok = check_gpu_info()
    
    # Check RAM
    check_system_ram()
    
    # Recommendations
    recommend_settings()
    
    if gpu_ok:
        # Test memory
        test_memory_usage()
    
    # Update configs
    update_training_configs()
    
    print("\nğŸ¯ Next Steps:")
    print("1. Run: python training/train_balanced.py")
    print("2. Monitor GPU usage: nvidia-smi")
    print("3. If still OOM, reduce batch_size to 2 or img_size to 512")

if __name__ == "__main__":
    main()