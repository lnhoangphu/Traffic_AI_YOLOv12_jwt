"""
Enhanced Dataset Analysis Tool - Ph√¢n t√≠ch chi ti·∫øt v√† ch√≠nh x√°c h∆°n
ƒê·ªçc ƒë∆∞·ª£c classes th·ª±c s·ª± t·ª´ YOLO labels v√† classification folders

Usage: python scripts/analyze_datasets_enhanced.py
"""

import os
import json
import yaml
from pathlib import Path
from collections import Counter, defaultdict
import xml.etree.ElementTree as ET
from tabulate import tabulate
import re

class EnhancedDatasetAnalyzer:
    def __init__(self, datasets_root="datasets_src"):
        self.datasets_root = Path(datasets_root)
        self.results = []
        
    def count_files_by_extension(self, directory, extensions):
        """ƒê·∫øm file theo extension trong th∆∞ m·ª•c"""
        if not directory.exists():
            return 0
        
        count = 0
        for ext in extensions:
            count += len(list(directory.glob(f"*.{ext}")))
            count += len(list(directory.glob(f"*.{ext.upper()}")))
        return count
    
    def read_classes_from_yolo_config(self, dataset_path):
        """ƒê·ªçc classes t·ª´ YOLO config files"""
        config_files = [
            dataset_path / 'data.yaml',
            dataset_path / 'dataset.yaml', 
            dataset_path / 'config.yaml',
            dataset_path / 'custom_data.yaml',
            dataset_path / 'intersection.yaml',
            dataset_path / 'classes.txt'
        ]
        
        for config_file in config_files:
            if config_file.exists():
                try:
                    print(f"      üîç Reading config: {config_file.name}")
                    if config_file.suffix == '.txt':
                        with open(config_file, 'r', encoding='utf-8') as f:
                            classes = [line.strip() for line in f if line.strip()]
                        if classes:
                            print(f"         Found {len(classes)} classes: {classes}")
                            return classes
                    else:
                        with open(config_file, 'r', encoding='utf-8') as f:
                            data = yaml.safe_load(f)
                            if data and 'names' in data:
                                if isinstance(data['names'], dict):
                                    class_list = [data['names'][i] for i in sorted(data['names'].keys())]
                                    print(f"         Found {len(class_list)} classes: {class_list}")
                                    return class_list
                                elif isinstance(data['names'], list):
                                    print(f"         Found {len(data['names'])} classes: {data['names']}")
                                    return data['names']
                except Exception as e:
                    print(f"      ‚ö†Ô∏è Error reading {config_file}: {e}")
        
        print(f"      ‚ö†Ô∏è No class config found in {dataset_path}")
        return []
    
    def analyze_yolo_labels_detailed(self, labels_dir):
        """Ph√¢n t√≠ch chi ti·∫øt YOLO labels"""
        if not labels_dir.exists():
            return set(), 0, {}
        
        class_counts = Counter()
        total_objects = 0
        files_with_labels = 0
        
        for label_file in labels_dir.glob("*.txt"):
            file_has_labels = False
            try:
                with open(label_file, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            parts = line.split()
                            if len(parts) >= 5:  # YOLO format: class x y w h
                                class_id = int(parts[0])
                                class_counts[class_id] += 1
                                total_objects += 1
                                file_has_labels = True
                if file_has_labels:
                    files_with_labels += 1
            except Exception:
                continue
        
        print(f"      üìä Found {total_objects} objects in {files_with_labels} files")
        if class_counts:
            print(f"         Classes found: {sorted(class_counts.keys())}")
            for class_id in sorted(class_counts.keys()):
                print(f"           Class {class_id}: {class_counts[class_id]} objects")
        
        return set(class_counts.keys()), total_objects, dict(class_counts)
    
    def analyze_classification_folders(self, data_path):
        """Ph√¢n t√≠ch folder-based classification ƒë·ªÉ t√¨m classes th·ª±c s·ª±"""
        img_extensions = ['jpg', 'jpeg', 'png', 'bmp', 'tiff', 'webp']
        
        # T√¨m c√°c th∆∞ m·ª•c con ch·ª©a ·∫£nh
        class_folders = []
        class_counts = {}
        
        def explore_directory(path, level=0):
            if level > 3:  # Gi·ªõi h·∫°n depth ƒë·ªÉ tr√°nh infinite loop
                return
            
            for item in path.iterdir():
                if item.is_dir():
                    # ƒê·∫øm ·∫£nh trong th∆∞ m·ª•c n√†y
                    img_count = sum(len(list(item.glob(f"*.{ext}"))) for ext in img_extensions)
                    img_count += sum(len(list(item.glob(f"*.{ext.upper()}"))) for ext in img_extensions)
                    
                    if img_count > 0:
                        # Th∆∞ m·ª•c n√†y ch·ª©a ·∫£nh - c√≥ th·ªÉ l√† class
                        parent_name = item.parent.name.lower()
                        if parent_name not in ['train', 'val', 'test', 'training', 'validation', 'testing', 'images']:
                            class_folders.append(item.name)
                            class_counts[item.name] = img_count
                    
                    # Ti·∫øp t·ª•c explore n·∫øu kh√¥ng ph·∫£i train/val/test
                    if item.name.lower() not in ['train', 'val', 'test', 'training', 'validation', 'testing']:
                        explore_directory(item, level + 1)
        
        explore_directory(data_path)
        
        return class_folders, class_counts
    
    def analyze_vn_traffic_sign(self, dataset_path):
        """Ph√¢n t√≠ch ƒë·∫∑c bi·ªát cho VN Traffic Sign dataset"""
        result = {
            'train_images': 0, 'val_images': 0, 'test_images': 0,
            'train_labels': 0, 'val_labels': 0, 'test_labels': 0,
            'classes': [], 'total_annotations': 0, 'class_distribution': {}
        }
        
        # ƒê·ªçc classes t·ª´ config
        classes = self.read_classes_from_yolo_config(dataset_path)
        if classes:
            result['classes'] = classes
        
        # Ph√¢n t√≠ch t·ª´ng split
        img_extensions = ['jpg', 'jpeg', 'png', 'bmp']
        
        for split in ['train', 'val', 'test']:
            img_dir = dataset_path / 'images' / split
            label_dir = dataset_path / 'labels' / split
            
            if img_dir.exists():
                result[f'{split}_images'] = self.count_files_by_extension(img_dir, img_extensions)
            
            if label_dir.exists():
                result[f'{split}_labels'] = self.count_files_by_extension(label_dir, ['txt'])
                
                # Ph√¢n t√≠ch classes trong labels
                classes_found, objects, class_counts = self.analyze_yolo_labels_detailed(label_dir)
                result['total_annotations'] += objects
                for class_id, count in class_counts.items():
                    result['class_distribution'][class_id] = result['class_distribution'].get(class_id, 0) + count
        
        return result
    
    def analyze_intersection_flow(self, dataset_path):
        """Ph√¢n t√≠ch ƒë·∫∑c bi·ªát cho Intersection Flow dataset"""
        result = {
            'train_images': 0, 'val_images': 0, 'test_images': 0,
            'train_labels': 0, 'val_labels': 0, 'test_labels': 0,
            'classes': [], 'total_annotations': 0, 'class_distribution': {}
        }
        
        # ƒê·ªçc classes t·ª´ config
        classes = self.read_classes_from_yolo_config(dataset_path)
        if classes:
            result['classes'] = classes
        
        # Ph√¢n t√≠ch YOLO structure
        img_extensions = ['jpg', 'jpeg', 'png', 'bmp']
        
        for split in ['train', 'val', 'test']:
            img_dir = dataset_path / 'images' / split
            label_dir = dataset_path / 'labels' / split
            
            if img_dir.exists():
                result[f'{split}_images'] = self.count_files_by_extension(img_dir, img_extensions)
            
            if label_dir.exists():
                result[f'{split}_labels'] = self.count_files_by_extension(label_dir, ['txt'])
                
                # Ph√¢n t√≠ch classes
                classes_found, objects, class_counts = self.analyze_yolo_labels_detailed(label_dir)
                result['total_annotations'] += objects
                for class_id, count in class_counts.items():
                    result['class_distribution'][class_id] = result['class_distribution'].get(class_id, 0) + count
        
        # N·∫øu kh√¥ng c√≥ classes t·ª´ config, t·∫°o t·ª´ labels
        if not result['classes'] and result['class_distribution']:
            max_class_id = max(result['class_distribution'].keys())
            result['classes'] = [f"class_{i}" for i in range(max_class_id + 1)]
        
        return result
    
    def analyze_road_issues(self, dataset_path):
        """Ph√¢n t√≠ch ƒë·∫∑c bi·ªát cho Road Issues dataset"""
        result = {
            'train_images': 0, 'val_images': 0, 'test_images': 0,
            'train_labels': 0, 'val_labels': 0, 'test_labels': 0,
            'classes': [], 'total_annotations': 0, 'class_distribution': {}
        }
        
        # Road Issues l√† classification dataset
        class_folders, class_counts = self.analyze_classification_folders(dataset_path)
        
        result['classes'] = class_folders
        result['train_images'] = sum(class_counts.values())
        result['class_distribution'] = class_counts
        
        return result
    
    def analyze_object_detection_35(self, dataset_path):
        """Ph√¢n t√≠ch ƒë·∫∑c bi·ªát cho Object Detection 35 dataset"""
        result = {
            'train_images': 0, 'val_images': 0, 'test_images': 0,
            'train_labels': 0, 'val_labels': 0, 'test_labels': 0,
            'classes': [], 'total_annotations': 0, 'class_distribution': {}
        }
        
        # Object Detection 35 c√≥ structure batch-based v·ªõi train/val/test splits
        img_extensions = ['jpg', 'jpeg', 'png', 'bmp']
        
        # Duy·ªát c√°c batch
        for batch_dir in dataset_path.glob("Batch *"):
            if batch_dir.is_dir():
                print(f"   üîç Processing {batch_dir.name}...")
                
                # Duy·ªát train/val/test trong m·ªói batch
                for split in ['train', 'val', 'test']:
                    img_dir = batch_dir / 'images' / split
                    label_dir = batch_dir / 'labels' / split
                    
                    if img_dir.exists():
                        batch_split_images = self.count_files_by_extension(img_dir, img_extensions)
                        result[f'{split}_images'] += batch_split_images
                        print(f"       {split}: {batch_split_images} images")
                    
                    if label_dir.exists():
                        batch_split_labels = self.count_files_by_extension(label_dir, ['txt'])
                        result[f'{split}_labels'] += batch_split_labels
                        
                        # Ph√¢n t√≠ch classes trong batch split n√†y
                        classes_found, objects, class_counts = self.analyze_yolo_labels_detailed(label_dir)
                        result['total_annotations'] += objects
                        for class_id, count in class_counts.items():
                            result['class_distribution'][class_id] = result['class_distribution'].get(class_id, 0) + count
        
        # T·∫°o class names d·ª±a tr√™n classes t√¨m ƒë∆∞·ª£c
        if result['class_distribution']:
            max_class_id = max(result['class_distribution'].keys())
            # Object Detection 35 l√† cutlery dataset v·ªõi 35 classes
            cutlery_classes = [
                'bowl', 'plate', 'fork', 'knife', 'spoon', 'cup', 'glass', 'chopsticks',
                'saucer', 'tray', 'napkin', 'placemat', 'tablecloth', 'coaster', 'pitcher',
                'teapot', 'sugar_bowl', 'cream_pitcher', 'salt_shaker', 'pepper_shaker',
                'butter_dish', 'serving_spoon', 'ladle', 'spatula', 'tongs', 'whisk',
                'cutting_board', 'rolling_pin', 'can_opener', 'bottle_opener', 'corkscrew',
                'grater', 'strainer', 'colander', 'mixer'
            ]
            
            # S·ª≠ d·ª•ng t√™n cutlery n·∫øu c√≥ ƒë·ªß, kh√¥ng th√¨ d√πng generic names
            if max_class_id < len(cutlery_classes):
                result['classes'] = cutlery_classes[:max_class_id + 1]
            else:
                result['classes'] = [f"cutlery_item_{i}" for i in range(max_class_id + 1)]
        
        return result
    
    def analyze_dataset(self, dataset_name, dataset_path):
        """Ph√¢n t√≠ch dataset v·ªõi logic ƒë·∫∑c bi·ªát cho t·ª´ng lo·∫°i"""
        print(f"\nüîç Analyzing {dataset_name}...")
        
        if dataset_name == 'vn_traffic_sign':
            actual_path = dataset_path / 'dataset'
            if actual_path.exists():
                result = self.analyze_vn_traffic_sign(actual_path)
            else:
                result = self.analyze_vn_traffic_sign(dataset_path)
        
        elif dataset_name == 'intersection_flow_5k':
            actual_path = dataset_path / 'Intersection-Flow-5K'
            if actual_path.exists():
                result = self.analyze_intersection_flow(actual_path)
            else:
                result = self.analyze_intersection_flow(dataset_path)
        
        elif dataset_name == 'road_issues':
            actual_path = dataset_path / 'data'
            if actual_path.exists():
                result = self.analyze_road_issues(actual_path)
            else:
                result = self.analyze_road_issues(dataset_path)
        
        elif dataset_name == 'object_detection_35':
            actual_path = dataset_path / 'final batches'
            if actual_path.exists():
                result = self.analyze_object_detection_35(actual_path)
            else:
                result = self.analyze_object_detection_35(dataset_path)
        
        else:
            # Generic analysis
            result = {
                'train_images': 0, 'val_images': 0, 'test_images': 0,
                'train_labels': 0, 'val_labels': 0, 'test_labels': 0,
                'classes': [], 'total_annotations': 0, 'class_distribution': {}
            }
        
        # Add metadata
        result['dataset'] = dataset_name
        result['path'] = str(dataset_path)
        
        # Add quality comments
        self.add_enhanced_comments(result)
        
        return result
    
    def add_enhanced_comments(self, result):
        """Th√™m nh·∫≠n x√©t chi ti·∫øt v·ªÅ dataset"""
        comments = []
        
        total_images = result['train_images'] + result['val_images'] + result['test_images']
        total_labels = result['train_labels'] + result['val_labels'] + result['test_labels']
        
        # Dataset size assessment
        if total_images == 0:
            comments.append("‚ùå Kh√¥ng c√≥ ·∫£nh")
        elif total_images < 100:
            comments.append("‚ö†Ô∏è Dataset nh·ªè (<100 ·∫£nh)")
        elif total_images < 1000:
            comments.append("üü° Dataset trung b√¨nh (<1k ·∫£nh)")
        elif total_images < 10000:
            comments.append("üü¢ Dataset kh√° l·ªõn (<10k ·∫£nh)")
        else:
            comments.append("‚úÖ Dataset l·ªõn (>10k ·∫£nh)")
        
        # Train/Val/Test split assessment
        if result['val_images'] > 0 and result['test_images'] > 0:
            comments.append("‚úÖ C√≥ ƒë·∫ßy ƒë·ªß train/val/test")
        elif result['val_images'] > 0:
            comments.append("‚úÖ C√≥ train/val split")
        elif result['test_images'] > 0:
            comments.append("üü° C√≥ train/test split")
        else:
            comments.append("‚ö†Ô∏è Ch·ªâ c√≥ train data")
        
        # Label coverage assessment
        if total_labels > 0:
            label_coverage = total_labels / max(total_images, 1)
            if label_coverage >= 0.95:
                comments.append("‚úÖ Labels ƒë·∫ßy ƒë·ªß")
            elif label_coverage >= 0.8:
                comments.append("üü¢ Labels kh√° ƒë·∫ßy ƒë·ªß")
            elif label_coverage >= 0.5:
                comments.append("üü° Thi·∫øu m·ªôt s·ªë labels")
            else:
                comments.append("‚ö†Ô∏è Thi·∫øu nhi·ªÅu labels")
        
        # Class assessment
        num_classes = len(result['classes'])
        if num_classes == 0:
            comments.append("‚ùå Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c classes")
        elif num_classes == 1:
            comments.append("‚ÑπÔ∏è Single-class dataset")
        elif num_classes <= 10:
            comments.append(f"‚úÖ {num_classes} classes (balanced)")
        elif num_classes <= 50:
            comments.append(f"üü° {num_classes} classes (multi-class)")
        else:
            comments.append(f"‚ö†Ô∏è {num_classes} classes (r·∫•t nhi·ªÅu)")
        
        # Annotation density
        if result['total_annotations'] > 0 and total_images > 0:
            annotations_per_image = result['total_annotations'] / total_images
            if annotations_per_image < 0.1:
                comments.append("‚ö†Ô∏è R·∫•t √≠t annotations/·∫£nh")
            elif annotations_per_image < 1:
                comments.append("üü° √çt annotations/·∫£nh")
            elif annotations_per_image <= 5:
                comments.append("‚úÖ Annotations/·∫£nh h·ª£p l√Ω")
            else:
                comments.append("‚ÑπÔ∏è Nhi·ªÅu objects/·∫£nh")
        
        # Class distribution assessment
        if result['class_distribution']:
            class_counts = list(result['class_distribution'].values())
            if len(class_counts) > 1:
                max_count = max(class_counts)
                min_count = min(class_counts)
                imbalance_ratio = max_count / max(min_count, 1)
                
                if imbalance_ratio > 10:
                    comments.append("‚ö†Ô∏è Classes m·∫•t c√¢n b·∫±ng nghi√™m tr·ªçng")
                elif imbalance_ratio > 3:
                    comments.append("üü° Classes h∆°i m·∫•t c√¢n b·∫±ng")
                else:
                    comments.append("‚úÖ Classes c√¢n b·∫±ng")
        
        result['comments'] = comments
    
    def analyze_all_datasets(self):
        """Ph√¢n t√≠ch t·∫•t c·∫£ datasets"""
        print("üöÄ ENHANCED DATASET ANALYSIS TOOL")
        print("=" * 70)
        
        if not self.datasets_root.exists():
            print(f"‚ùå Th∆∞ m·ª•c {self.datasets_root} kh√¥ng t·ªìn t·∫°i!")
            return
        
        # Find dataset directories
        dataset_dirs = [d for d in self.datasets_root.iterdir() if d.is_dir()]
        
        if not dataset_dirs:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y dataset n√†o trong {self.datasets_root}")
            return
        
        print(f"üìÅ T√¨m th·∫•y {len(dataset_dirs)} datasets:")
        for d in dataset_dirs:
            print(f"   - {d.name}")
        
        # Analyze each dataset
        for dataset_dir in dataset_dirs:
            result = self.analyze_dataset(dataset_dir.name, dataset_dir)
            self.results.append(result)
        
        # Generate comprehensive report
        self.generate_comprehensive_report()
    
    def generate_comprehensive_report(self):
        """T·∫°o b√°o c√°o to√†n di·ªán"""
        print("\nüìä COMPREHENSIVE DATASET REPORT")
        print("=" * 80)
        
        # Summary table
        table_data = []
        for result in self.results:
            total_images = result['train_images'] + result['val_images'] + result['test_images']
            total_labels = result['train_labels'] + result['val_labels'] + result['test_labels']
            
            # Format image breakdown
            img_detail = f"{total_images}"
            if result['val_images'] > 0 or result['test_images'] > 0:
                img_detail += f" (T:{result['train_images']}/V:{result['val_images']}/Te:{result['test_images']})"
            
            # Format label info
            label_detail = f"{total_labels}"
            if total_labels > 0:
                coverage = (total_labels / max(total_images, 1)) * 100
                label_detail += f" ({coverage:.0f}%)"
            
            # Classes summary
            num_classes = len(result['classes'])
            if num_classes <= 5 and result['classes']:
                class_summary = f"{num_classes}: {', '.join(result['classes'][:3])}"
                if num_classes > 3:
                    class_summary += "..."
            else:
                class_summary = f"{num_classes} classes"
            
            # Top comments
            top_comments = "; ".join(result['comments'][:2])
            
            table_data.append([
                result['dataset'],
                img_detail,
                label_detail,
                class_summary,
                result['total_annotations'],
                top_comments
            ])
        
        headers = ["Dataset", "Images (T/V/Te)", "Labels (%)", "Classes", "Annotations", "Status"]
        print(tabulate(table_data, headers=headers, tablefmt="grid", maxcolwidths=[15, 18, 15, 25, 12, 35]))
        
        # Detailed breakdown for each dataset
        self.print_detailed_analysis()
    
    def print_detailed_analysis(self):
        """In ph√¢n t√≠ch chi ti·∫øt cho t·ª´ng dataset"""
        print("\nüìã DETAILED ANALYSIS")
        print("=" * 80)
        
        for result in self.results:
            print(f"\nüéØ {result['dataset'].upper()}")
            print(f"   üìÅ Path: {result['path']}")
            
            # Image statistics
            total_images = result['train_images'] + result['val_images'] + result['test_images']
            print(f"   üñºÔ∏è Images: {total_images} total")
            if result['train_images'] > 0:
                print(f"       ‚îî‚îÄ‚îÄ Train: {result['train_images']}")
            if result['val_images'] > 0:
                print(f"       ‚îî‚îÄ‚îÄ Val: {result['val_images']}")
            if result['test_images'] > 0:
                print(f"       ‚îî‚îÄ‚îÄ Test: {result['test_images']}")
            
            # Label statistics
            total_labels = result['train_labels'] + result['val_labels'] + result['test_labels']
            if total_labels > 0:
                print(f"   üè∑Ô∏è Labels: {total_labels} total ({(total_labels/max(total_images,1)*100):.1f}% coverage)")
                print(f"   üìä Annotations: {result['total_annotations']} objects ({result['total_annotations']/max(total_images,1):.1f} obj/img)")
            
            # Classes
            print(f"   üéØ Classes ({len(result['classes'])}):")
            if result['classes']:
                for i, class_name in enumerate(result['classes'][:10]):
                    count = result['class_distribution'].get(i, 0) if isinstance(list(result['class_distribution'].keys())[0] if result['class_distribution'] else 0, int) else result['class_distribution'].get(class_name, 0)
                    print(f"       {i}: {class_name} ({count} samples)")
                if len(result['classes']) > 10:
                    print(f"       ... v√† {len(result['classes']) - 10} classes kh√°c")
            else:
                print("       Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c classes")
            
            # Comments
            print(f"   üí≠ Assessment:")
            for comment in result['comments']:
                print(f"       {comment}")

def main():
    analyzer = EnhancedDatasetAnalyzer()
    analyzer.analyze_all_datasets()
    
    print("\nüéâ Enhanced analysis complete!")
    print("\nüìö TRAINING RECOMMENDATIONS:")
    print("=" * 50)
    
    for result in analyzer.results:
        print(f"\nüéØ {result['dataset']}:")
        if result['dataset'] == 'vn_traffic_sign':
            print("   ‚úÖ Ready for YOLO training - Good structure")
            print("   üìù Use: python scripts/train_vn_traffic_sign.py")
        elif result['dataset'] == 'intersection_flow_5k':
            print("   ‚úÖ Ready for YOLO training - Multi-class traffic")
            print("   üìù Use: python scripts/train_intersection_flow.py")
        elif result['dataset'] == 'road_issues':
            print("   üîÑ Needs conversion to YOLO format")
            print("   üìù Use: python scripts/train_road_issues.py")
        elif result['dataset'] == 'object_detection_35':
            print("   ‚ö†Ô∏è Not traffic-related (cutlery dataset)")
            print("   üìù Skip or use for testing only")

if __name__ == "__main__":
    main()