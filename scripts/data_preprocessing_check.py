#!/usr/bin/env python3
"""
Comprehensive Data Preprocessing Check for Traffic AI Dataset
Ki·ªÉm tra c√°c v·∫•n ƒë·ªÅ ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu tr∆∞·ªõc khi hu·∫•n luy·ªán
"""

import os
import cv2
import numpy as np
from pathlib import Path
import yaml
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import seaborn as sns

class DataPreprocessingChecker:
    def __init__(self, dataset_path):
        self.dataset_path = Path(dataset_path)
        self.splits = ['train', 'val', 'test']
        self.issues = []
        self.stats = {}
        
    def load_data_yaml(self):
        """Load data.yaml configuration"""
        data_yaml = self.dataset_path / 'data.yaml'
        if data_yaml.exists():
            with open(data_yaml, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        return None
    
    def check_image_sizes(self):
        """1. Ki·ªÉm tra k√≠ch th∆∞·ªõc ·∫£nh v√† t√≠nh nh·∫•t qu√°n"""
        print("üîç 1. KI·ªÇM TRA K√çCH TH∆Ø·ªöC ·∫¢NH")
        print("=" * 50)
        
        sizes = []
        corrupted_images = []
        size_distribution = Counter()
        
        for split in self.splits:
            images_dir = self.dataset_path / 'images' / split
            if not images_dir.exists():
                continue
                
            print(f"üìÇ Checking {split} images...")
            image_files = list(images_dir.glob('*.jpg')) + list(images_dir.glob('*.png'))
            
            for img_path in image_files:
                try:
                    img = cv2.imread(str(img_path))
                    if img is None:
                        corrupted_images.append(str(img_path))
                        continue
                    
                    h, w = img.shape[:2]
                    sizes.append((w, h))
                    size_distribution[(w, h)] += 1
                    
                except Exception as e:
                    corrupted_images.append(f"{img_path}: {str(e)}")
        
        if sizes:
            sizes_array = np.array(sizes)
            unique_sizes = len(size_distribution)
            most_common_size = size_distribution.most_common(1)[0]
            
            print(f"üìä T·ªïng s·ªë ·∫£nh: {len(sizes)}")
            print(f"üìè K√≠ch th∆∞·ªõc trung b√¨nh: {sizes_array.mean(axis=0).astype(int)}")
            print(f"üìè K√≠ch th∆∞·ªõc ph·ªï bi·∫øn nh·∫•t: {most_common_size[0]} (xu·∫•t hi·ªán {most_common_size[1]} l·∫ßn)")
            print(f"üî¢ S·ªë lo·∫°i k√≠ch th∆∞·ªõc kh√°c nhau: {unique_sizes}")
            
            if unique_sizes > 10:
                self.issues.append(f"‚ùå K√≠ch th∆∞·ªõc ·∫£nh kh√¥ng ƒë·ªìng nh·∫•t ({unique_sizes} lo·∫°i kh√°c nhau)")
            else:
                print("‚úÖ K√≠ch th∆∞·ªõc ·∫£nh t∆∞∆°ng ƒë·ªëi ƒë·ªìng nh·∫•t")
        
        if corrupted_images:
            print(f"üö® Ph√°t hi·ªán {len(corrupted_images)} ·∫£nh l·ªói:")
            for img in corrupted_images[:5]:  # Show first 5
                print(f"   - {img}")
            if len(corrupted_images) > 5:
                print(f"   ... v√† {len(corrupted_images) - 5} ·∫£nh kh√°c")
            self.issues.append(f"‚ùå {len(corrupted_images)} ·∫£nh b·ªã l·ªói")
        
        self.stats['total_images'] = len(sizes)
        self.stats['corrupted_images'] = len(corrupted_images)
        self.stats['size_variations'] = unique_sizes
        
    def check_annotations(self):
        """2. Ki·ªÉm tra annotation YOLO format"""
        print("\nüîç 2. KI·ªÇM TRA ANNOTATION YOLO")
        print("=" * 50)
        
        missing_labels = []
        invalid_labels = []
        class_distribution = Counter()
        bbox_issues = []
        
        data_config = self.load_data_yaml()
        num_classes = data_config.get('nc', 0) if data_config else 0
        
        for split in self.splits:
            images_dir = self.dataset_path / 'images' / split
            labels_dir = self.dataset_path / 'labels' / split
            
            if not images_dir.exists() or not labels_dir.exists():
                continue
                
            print(f"üìÇ Checking {split} annotations...")
            image_files = list(images_dir.glob('*.jpg')) + list(images_dir.glob('*.png'))
            
            for img_path in image_files:
                label_path = labels_dir / f"{img_path.stem}.txt"
                
                # Check if label file exists
                if not label_path.exists():
                    missing_labels.append(str(img_path))
                    continue
                
                # Check label content
                try:
                    with open(label_path, 'r') as f:
                        lines = f.readlines()
                    
                    for line_num, line in enumerate(lines, 1):
                        line = line.strip()
                        if not line:
                            continue
                            
                        parts = line.split()
                        if len(parts) != 5:
                            invalid_labels.append(f"{label_path}:{line_num} - Wrong format")
                            continue
                        
                        try:
                            class_id = int(parts[0])
                            x_center, y_center, width, height = map(float, parts[1:])
                            
                            # Check class ID range
                            if class_id < 0 or (num_classes > 0 and class_id >= num_classes):
                                invalid_labels.append(f"{label_path}:{line_num} - Invalid class ID: {class_id}")
                            
                            # Check bbox coordinates (should be 0-1)
                            if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and 
                                   0 <= width <= 1 and 0 <= height <= 1):
                                bbox_issues.append(f"{label_path}:{line_num} - Bbox out of range")
                            
                            class_distribution[class_id] += 1
                            
                        except ValueError:
                            invalid_labels.append(f"{label_path}:{line_num} - Invalid numbers")
                            
                except Exception as e:
                    invalid_labels.append(f"{label_path}: {str(e)}")
        
        print(f"üìä Ph√¢n b·ªë classes:")
        if data_config and 'names' in data_config:
            for class_id in sorted(class_distribution.keys()):
                class_name = data_config['names'][class_id] if class_id < len(data_config['names']) else f"Class_{class_id}"
                print(f"   {class_id}: {class_name} - {class_distribution[class_id]} objects")
        
        if missing_labels:
            print(f"üö® {len(missing_labels)} ·∫£nh thi·∫øu annotation")
            self.issues.append(f"‚ùå {len(missing_labels)} ·∫£nh thi·∫øu annotation")
        
        if invalid_labels:
            print(f"üö® {len(invalid_labels)} annotation l·ªói format")
            for issue in invalid_labels[:5]:
                print(f"   - {issue}")
            if len(invalid_labels) > 5:
                print(f"   ... v√† {len(invalid_labels) - 5} l·ªói kh√°c")
            self.issues.append(f"‚ùå {len(invalid_labels)} annotation l·ªói format")
        
        if bbox_issues:
            print(f"üö® {len(bbox_issues)} bbox coordinates l·ªói")
            self.issues.append(f"‚ùå {len(bbox_issues)} bbox coordinates l·ªói")
        
        self.stats['missing_labels'] = len(missing_labels)
        self.stats['invalid_labels'] = len(invalid_labels)
        self.stats['bbox_issues'] = len(bbox_issues)
        self.stats['class_distribution'] = dict(class_distribution)
        
    def check_data_balance(self):
        """3. Ki·ªÉm tra c√¢n b·∫±ng d·ªØ li·ªáu"""
        print("\nüîç 3. PH√ÇN T√çCH C√ÇN B·∫∞NG D·ªÆ LI·ªÜU")
        print("=" * 50)
        
        if 'class_distribution' not in self.stats:
            print("‚ùå Kh√¥ng th·ªÉ ph√¢n t√≠ch do thi·∫øu th√¥ng tin class distribution")
            return
        
        class_counts = list(self.stats['class_distribution'].values())
        if not class_counts:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu class n√†o")
            return
        
        min_count = min(class_counts)
        max_count = max(class_counts)
        mean_count = np.mean(class_counts)
        std_count = np.std(class_counts)
        cv = std_count / mean_count if mean_count > 0 else 0
        
        print(f"üìä S·ªë l∆∞·ª£ng object:")
        print(f"   - √çt nh·∫•t: {min_count}")
        print(f"   - Nhi·ªÅu nh·∫•t: {max_count}")
        print(f"   - Trung b√¨nh: {mean_count:.1f}")
        print(f"   - T·ª∑ l·ªá imbalance: {max_count/min_count:.2f}:1")
        print(f"   - Coefficient of Variation: {cv:.3f}")
        
        if cv > 0.5:
            self.issues.append(f"‚ùå D·ªØ li·ªáu m·∫•t c√¢n b·∫±ng nghi√™m tr·ªçng (CV = {cv:.3f})")
            print(f"üö® D·ªØ li·ªáu m·∫•t c√¢n b·∫±ng nghi√™m tr·ªçng (CV = {cv:.3f})")
        elif cv > 0.3:
            self.issues.append(f"‚ö†Ô∏è D·ªØ li·ªáu h∆°i m·∫•t c√¢n b·∫±ng (CV = {cv:.3f})")
            print(f"‚ö†Ô∏è D·ªØ li·ªáu h∆°i m·∫•t c√¢n b·∫±ng (CV = {cv:.3f})")
        else:
            print(f"‚úÖ D·ªØ li·ªáu t∆∞∆°ng ƒë·ªëi c√¢n b·∫±ng (CV = {cv:.3f})")
        
        self.stats['imbalance_ratio'] = max_count/min_count if min_count > 0 else float('inf')
        self.stats['cv'] = cv
    
    def check_augmentation_potential(self):
        """4. ƒê√°nh gi√° kh·∫£ nƒÉng augmentation"""
        print("\nüîç 4. ƒê√ÅNH GI√Å AUGMENTATION")
        print("=" * 50)
        
        # Sample some images to check diversity
        train_images_dir = self.dataset_path / 'images' / 'train'
        if not train_images_dir.exists():
            print("‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c train images")
            return
        
        image_files = list(train_images_dir.glob('*.jpg'))[:50]  # Sample 50 images
        if not image_files:
            print("‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh training")
            return
        
        brightness_values = []
        contrast_values = []
        
        for img_path in image_files:
            try:
                img = cv2.imread(str(img_path))
                if img is None:
                    continue
                
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                brightness = np.mean(gray)
                contrast = np.std(gray)
                
                brightness_values.append(brightness)
                contrast_values.append(contrast)
                
            except Exception:
                continue
        
        if brightness_values:
            brightness_std = np.std(brightness_values)
            contrast_std = np.std(contrast_values)
            
            print(f"üìä ƒêa d·∫°ng v·ªÅ brightness: {brightness_std:.2f}")
            print(f"üìä ƒêa d·∫°ng v·ªÅ contrast: {contrast_std:.2f}")
            
            print(f"üí° C√°c k·ªπ thu·∫≠t augmentation ƒë∆∞·ª£c khuy·∫øn ngh·ªã:")
            print(f"   - Random rotation (¬±10-15¬∞)")
            print(f"   - Random scaling (0.8-1.2)")
            print(f"   - Brightness adjustment (¬±20%)")
            print(f"   - Random horizontal flip")
            print(f"   - Color jittering")
            print(f"   - Random crop/zoom")
        
    def generate_recommendations(self):
        """5. ƒê∆∞a ra khuy·∫øn ngh·ªã c·∫£i thi·ªán"""
        print("\nüéØ 5. KHUY·∫æN NGH·ªä TI·ªÄN X·ª¨ L√ù")
        print("=" * 50)
        
        if not self.issues:
            print("‚úÖ D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng cho training!")
            return
        
        print("üîß C√°c v·∫•n ƒë·ªÅ c·∫ßn kh·∫Øc ph·ª•c:")
        for issue in self.issues:
            print(f"   {issue}")
        
        print("\nüí° Khuy·∫øn ngh·ªã c·ª• th·ªÉ:")
        
        # Image size recommendations
        if self.stats.get('size_variations', 0) > 10:
            print("üìè Chu·∫©n h√≥a k√≠ch th∆∞·ªõc ·∫£nh:")
            print("   - Resize t·∫•t c·∫£ ·∫£nh v·ªÅ 640x640 (ho·∫∑c 512x512)")
            print("   - S·ª≠ d·ª•ng padding ƒë·ªÉ gi·ªØ aspect ratio")
        
        # Corruption recommendations
        if self.stats.get('corrupted_images', 0) > 0:
            print("üîß X·ª≠ l√Ω ·∫£nh l·ªói:")
            print("   - Lo·∫°i b·ªè ho·∫∑c thay th·∫ø ·∫£nh b·ªã corrupted")
            print("   - Ki·ªÉm tra l·∫°i qu√° tr√¨nh download/convert")
        
        # Annotation recommendations
        if self.stats.get('missing_labels', 0) > 0:
            print("üìù X·ª≠ l√Ω annotation thi·∫øu:")
            print("   - Lo·∫°i b·ªè ·∫£nh kh√¥ng c√≥ label")
            print("   - Ho·∫∑c t·∫°o label m·ªõi cho nh·ªØng ·∫£nh quan tr·ªçng")
        
        # Balance recommendations
        if self.stats.get('cv', 0) > 0.5:
            print("‚öñÔ∏è C√¢n b·∫±ng d·ªØ li·ªáu:")
            print("   - Oversampling cho classes √≠t")
            print("   - Data augmentation ƒë·∫∑c bi·ªát cho classes thi·∫øu")
            print("   - Class weighting trong loss function")
            print("   - Focal loss ƒë·ªÉ x·ª≠ l√Ω imbalance")
    
    def run_full_check(self):
        """Ch·∫°y t·∫•t c·∫£ c√°c ki·ªÉm tra"""
        print(f"üîç KI·ªÇM TRA TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU")
        print(f"Dataset: {self.dataset_path.name}")
        print("=" * 60)
        
        self.check_image_sizes()
        self.check_annotations()
        self.check_data_balance()
        self.check_augmentation_potential()
        self.generate_recommendations()
        
        print(f"\nüìã T√ìM T·∫ÆT KI·ªÇM TRA")
        print("=" * 60)
        print(f"üìä T·ªïng s·ªë v·∫•n ƒë·ªÅ: {len(self.issues)}")
        
        if not self.issues:
            print("üéâ D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng cho training!")
        else:
            print("‚ö†Ô∏è C·∫ßn x·ª≠ l√Ω c√°c v·∫•n ƒë·ªÅ tr∆∞·ªõc khi training.")

def main():
    """Main function"""
    base_dir = Path(r"d:\DH_K47\nam_tu\HK1\Do_an_2\Traffic_AI_YOLOv12_jwt\datasets")
    
    datasets = {
        "Balanced": base_dir / "traffic_ai_balanced_11class",
        "Imbalanced": base_dir / "traffic_ai_imbalanced_11class"
    }
    
    for name, dataset_path in datasets.items():
        if dataset_path.exists():
            print(f"\n{'='*20} {name} Dataset {'='*20}")
            checker = DataPreprocessingChecker(dataset_path)
            checker.run_full_check()
        else:
            print(f"‚ùå Dataset kh√¥ng t·ªìn t·∫°i: {dataset_path}")

if __name__ == "__main__":
    main()