from fastapi import FastAPI, File, UploadFile, HTTPException, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from .detect import infer, model, MODEL_PATH, analyze_video_to_json, save_annotated_video
import os
import time
import psutil
import torch
from pathlib import Path

import cv2
import numpy as np
import tempfile
from typing import Optional

# Load environment variables v·ªõi default values
API_TITLE = os.getenv("API_TITLE", "Traffic AI Service - YOLOv12")
API_VERSION = os.getenv("API_VERSION", "2.0.0")
TRAINING_DATASET_NAME = os.getenv("TRAINING_DATASET_NAME", "Traffic AI Balanced Dataset")

# T·∫°o app v·ªõi th√¥ng tin chi ti·∫øt t·ª´ .env
app = FastAPI(
    title=API_TITLE,
    description=f"API ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng giao th√¥ng s·ª≠ d·ª•ng YOLOv12 ƒë√£ ƒë∆∞·ª£c train tr√™n {TRAINING_DATASET_NAME}",
    version=API_VERSION,
    docs_url="/docs",
    redoc_url="/redoc"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=os.getenv("AI_ALLOWED_ORIGINS", "*").split(","),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# C·∫•u h√¨nh t·ª´ .env
MAX_SIZE = int(os.getenv("MAX_FILE_SIZE_MB", "10")) * 1024 * 1024  # Convert MB to bytes
REQUIRE_AUTH = os.getenv("REQUIRE_AUTH", "false").lower() == "true"
HIGH_CONFIDENCE_THRESHOLD = float(os.getenv("HIGH_CONFIDENCE_THRESHOLD", "0.7"))
BENCHMARK_MAX_SAMPLES = int(os.getenv("BENCHMARK_MAX_SAMPLES", "50"))
DEFAULT_BENCHMARK_SAMPLES = int(os.getenv("DEFAULT_BENCHMARK_SAMPLES", "10"))
WARMUP_ITERATIONS = int(os.getenv("WARMUP_ITERATIONS", "3"))
SHOW_TRAINING_METRICS = os.getenv("SHOW_TRAINING_METRICS", "true").lower() == "true"
ENABLE_GPU_METRICS = os.getenv("ENABLE_GPU_METRICS", "true").lower() == "true"

def _auth(authorization: str | None):
    """X√°c th·ª±c ng∆∞·ªùi d√πng"""
    if REQUIRE_AUTH:
        if not authorization or not authorization.startswith("Bearer "):
            raise HTTPException(status_code=401, detail="Thi·∫øu ho·∫∑c sai token x√°c th·ª±c")
        # TODO: verify JWT signature (HS256/RS256) t√πy backend c·ªßa b·∫°n

@app.get("/")
async def root():
    """Trang ch·ªß API v·ªõi danh s√°ch endpoints"""
    return {
        "message": f"üöÄ Ch√†o m·ª´ng ƒë·∫øn v·ªõi {API_TITLE}",
        "model_info": f"YOLOv12 ƒë√£ ƒë∆∞·ª£c train tr√™n {TRAINING_DATASET_NAME}",
        "version": API_VERSION,
        "config": {
            "max_file_size_mb": int(os.getenv("MAX_FILE_SIZE_MB", "10")),
            "auth_required": REQUIRE_AUTH,
            "high_confidence_threshold": HIGH_CONFIDENCE_THRESHOLD
        },
        "endpoints": {
            "/detect": "POST - T·∫£i l√™n ·∫£nh ƒë·ªÉ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng giao th√¥ng",
            "/model/info": "GET - Th√¥ng tin chi ti·∫øt v·ªÅ model",
            "/model/metrics": "GET - Hi·ªáu su·∫•t h·ªá th·ªëng hi·ªán t·∫°i", 
            "/test/benchmark": "POST - ƒêo t·ªëc ƒë·ªô inference",
            "/healthz": "GET - Ki·ªÉm tra tr·∫°ng th√°i API",
            "/docs": "üìö T√†i li·ªáu API t∆∞∆°ng t√°c (Swagger UI)",
            "/redoc": "üìñ T√†i li·ªáu API (ReDoc)"
        },
        "classes": [
            "Vehicle", "Bus", "Bicycle", "Person", "Engine", 
            "Truck", "Tricycle", "Obstacle", "Pothole", 
            "Traffic Light", "Traffic Sign"
        ]
    }

@app.get("/healthz")
def healthz():
    """Ki·ªÉm tra tr·∫°ng th√°i API"""
    return {
        "status": "ok", 
        "timestamp": time.time(),
        "model_loaded": model is not None,
        "model_path": MODEL_PATH
    }

@app.get("/model/info")
async def model_info():
    """Th√¥ng tin chi ti·∫øt v·ªÅ model"""
    try:
        # L·∫•y training metrics t·ª´ .env n·∫øu c√≥
        performance_data = {}
        if SHOW_TRAINING_METRICS:
            performance_data = {
                "mAP@0.5": f"{os.getenv('TRAINING_MAP50', '62.8')}%",
                "mAP@0.5:0.95": f"{os.getenv('TRAINING_MAP50_95', '44.9')}%",
                "precision": f"{os.getenv('TRAINING_PRECISION', '79.6')}%",
                "recall": f"{os.getenv('TRAINING_RECALL', '55.7')}%"
            }
        
        return {
            "model_type": "YOLOv12n",
            "training_dataset": TRAINING_DATASET_NAME,
            "classes": list(model.names.values()) if hasattr(model, 'names') else [],
            "num_classes": len(model.names) if hasattr(model, 'names') else 0,
            "input_size": "640x640",
            "model_path": MODEL_PATH,
            "performance": performance_data,
            "thresholds": {
                "confidence": float(os.getenv("MODEL_CONFIDENCE_THRESHOLD", "0.25")),
                "iou": float(os.getenv("MODEL_IOU_THRESHOLD", "0.45")),
                "high_confidence": HIGH_CONFIDENCE_THRESHOLD,
                "person_min_conf": float(os.getenv("PERSON_MIN_CONF", "0.75")),
                "vehicle_min_conf": float(os.getenv("VEHICLE_MIN_CONF", "0.20")),
                "suppress_person_if_iou_with_vehicle": float(os.getenv("SUPPRESS_PERSON_IF_IOU_WITH_VEHICLE", "0.6"))
            },
            "training_info": {
                "epochs": 100,
                "batch_size": 4,
                "optimizer": "AdamW",
                "device": "CUDA (RTX 3050 Ti)"
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói l·∫•y th√¥ng tin model: {str(e)}")

@app.get("/model/metrics")
async def model_metrics():
    """Th·ªëng k√™ hi·ªáu su·∫•t h·ªá th·ªëng"""
    try:

    # API x·ª≠ l√Ω video: t√°ch frame, detect t·ª´ng frame, tr·∫£ v·ªÅ k·∫øt qu·∫£ JSON
        # Th√¥ng tin b·ªô nh·ªõ
        process = psutil.Process()
        memory_info = process.memory_info()
        
        # Th√¥ng tin GPU n·∫øu c√≥ v√† ƒë∆∞·ª£c b·∫≠t
        gpu_info = {"gpu_available": False}
        if ENABLE_GPU_METRICS and torch.cuda.is_available():
            gpu_info = {
                "gpu_available": True,
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_total_gb": round(torch.cuda.get_device_properties(0).total_memory / 1e9, 2),
                "gpu_memory_allocated_mb": round(torch.cuda.memory_allocated(0) / 1e6, 2),
                "gpu_memory_cached_mb": round(torch.cuda.memory_reserved(0) / 1e6, 2)
            }
        
        return {
            "system": {
                "ram_usage_mb": round(memory_info.rss / 1024 / 1024, 2),
                "cpu_percent": psutil.cpu_percent(),
                "cpu_count": psutil.cpu_count()
            },
            "gpu": gpu_info,
            "config": {
                "gpu_metrics_enabled": ENABLE_GPU_METRICS,
                "max_file_size_mb": int(os.getenv("MAX_FILE_SIZE_MB", "10")),
                "benchmark_max_samples": BENCHMARK_MAX_SAMPLES
            },
            "timestamp": time.time()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói l·∫•y th√¥ng tin h·ªá th·ªëng: {str(e)}")

@app.post("/test/benchmark")
async def benchmark_inference(num_samples: int = DEFAULT_BENCHMARK_SAMPLES):
    """ƒêo t·ªëc ƒë·ªô inference c·ªßa model"""
    if num_samples > BENCHMARK_MAX_SAMPLES:
        raise HTTPException(
            status_code=400, 
            detail=f"T·ªëi ƒëa {BENCHMARK_MAX_SAMPLES} m·∫´u test (c√≥ th·ªÉ thay ƒë·ªïi trong .env)"
        )
    
    try:
        import io
        import numpy as np
        from PIL import Image
        
        # T·∫°o ·∫£nh test
        test_img = Image.new('RGB', (640, 640), color=(128, 128, 128))
        img_bytes = io.BytesIO()
        test_img.save(img_bytes, format='JPEG')
        test_data = img_bytes.getvalue()
        
        # Warm-up v·ªõi s·ªë l·∫ßn t·ª´ .env
        for _ in range(WARMUP_ITERATIONS):
            infer(test_data)
        
        # ƒêo t·ªëc ƒë·ªô
        times = []
        for i in range(num_samples):
            start_time = time.time()
            results = infer(test_data)
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # chuy·ªÉn sang ms
        
        avg_time = np.mean(times)
        
        return {
            "test_info": {
                "num_samples": num_samples,
                "warmup_iterations": WARMUP_ITERATIONS,
                "image_size": "640x640",
                "model": f"YOLOv12n ({TRAINING_DATASET_NAME})"
            },
            "performance": {
                "avg_inference_time_ms": round(avg_time, 2),
                "min_time_ms": round(np.min(times), 2),
                "max_time_ms": round(np.max(times), 2),
                "std_time_ms": round(np.std(times), 2),
                "throughput_fps": round(1000 / avg_time, 2)
            },
            "status": "‚úÖ T·ªët" if avg_time < 100 else "‚ö†Ô∏è Ch·∫≠m",
            "config": {
                "max_samples_allowed": BENCHMARK_MAX_SAMPLES,
                "default_samples": DEFAULT_BENCHMARK_SAMPLES
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói benchmark: {str(e)}")

@app.post("/video/detect")
async def video_detect(
    file: UploadFile = File(...),
    sample_every: int = 1,
    max_frames: Optional[int] = 300,
    authorization: str | None = Header(None)
):
    """
    Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng theo t·ª´ng frame trong video v√† tr·∫£ v·ªÅ JSON.
    - sample_every: l·∫•y m·∫´u m·ªói N frame (m·∫∑c ƒë·ªãnh 1: m·ªçi frame)
    - max_frames: gi·ªõi h·∫°n s·ªë frame x·ª≠ l√Ω ƒë·ªÉ ph·∫£n h·ªìi nhanh
    """
    _auth(authorization)

    if file.content_type not in ("video/mp4", "video/quicktime", "application/octet-stream"):
        raise HTTPException(status_code=400, detail="Ch·ªâ h·ªó tr·ª£ video MP4/MOV")

    data = await file.read()
    if len(data) > (MAX_SIZE * 5):  # Cho ph√©p video l·ªõn h∆°n ·∫£nh m·ªôt ch√∫t
        raise HTTPException(status_code=413, detail="Video qu√° l·ªõn. T·ªëi ƒëa ~50MB (t√πy c·∫•u h√¨nh)")

    try:
        result = analyze_video_to_json(data, sample_every=sample_every, max_frames=max_frames)
        return {
            "success": True,
            "video": {
                "filename": file.filename,
                "size_bytes": len(data),
                "content_type": file.content_type
            },
            "model": {
                "name": "YOLOv12n Balanced",
                "conf": float(os.getenv("MODEL_CONFIDENCE_THRESHOLD", "0.25")),
                "iou": float(os.getenv("MODEL_IOU_THRESHOLD", "0.45"))
            },
            "result": result,
            "timestamp": time.time()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω video: {str(e)}")

@app.post("/video/stream")
async def video_stream(
    file: UploadFile = File(...),
    sample_every: int = 1,
    max_frames: Optional[int] = None,
    output_fps: Optional[float] = None,
    authorization: str | None = Header(None)
):
    """
    Tr·∫£ v·ªÅ video MP4 ƒë√£ ƒë∆∞·ª£c v·∫Ω bounding boxes.
    """
    _auth(authorization)

    if file.content_type not in ("video/mp4", "video/quicktime", "application/octet-stream"):
        raise HTTPException(status_code=400, detail="Ch·ªâ h·ªó tr·ª£ video MP4/MOV")

    data = await file.read()
    if len(data) > (MAX_SIZE * 10):
        raise HTTPException(status_code=413, detail="Video qu√° l·ªõn. T·ªëi ƒëa ~100MB (t√πy c·∫•u h√¨nh)")

    try:
        with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmp_out:
            out_path = tmp_out.name

        info = save_annotated_video(
            data,
            output_path=out_path,
            sample_every=sample_every,
            max_frames=max_frames,
            output_fps=output_fps
        )

        def iterfile():
            try:
                with open(out_path, "rb") as f:
                    while True:
                        chunk = f.read(1024 * 1024)
                        if not chunk:
                            break
                        yield chunk
            finally:
                try:
                    os.remove(out_path)
                except Exception:
                    pass

        headers = {
            "X-Video-Width": str(info["width"]),
            "X-Video-Height": str(info["height"]),
            "X-Video-FPS": str(info["fps"]),
        }

        return StreamingResponse(iterfile(), media_type="video/mp4", headers=headers)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói t·∫°o video annotate: {str(e)}")

@app.post("/detect")
async def detect(file: UploadFile = File(...), authorization: str | None = Header(None)):
    """
    Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng giao th√¥ng trong ·∫£nh
    
    - **file**: ·∫¢nh c·∫ßn ph√¢n t√≠ch (JPEG/PNG, t·ªëi ƒëa 10MB)
    - **authorization**: Token x√°c th·ª±c (n·∫øu b·∫Øt bu·ªôc)
    
    Tr·∫£ v·ªÅ danh s√°ch c√°c ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c ph√°t hi·ªán v·ªõi t·ªça ƒë·ªô v√† ƒë·ªô tin c·∫≠y
    """
    _auth(authorization)
    
    # Ki·ªÉm tra ƒë·ªãnh d·∫°ng file
    if file.content_type not in ("image/jpeg", "image/png"):
        raise HTTPException(
            status_code=400, 
            detail="Ch·ªâ h·ªó tr·ª£ ·∫£nh JPEG v√† PNG"
        )
    
    # ƒê·ªçc file
    data = await file.read()
    if len(data) > MAX_SIZE:
        raise HTTPException(
            status_code=413, 
            detail=f"File qu√° l·ªõn. T·ªëi ƒëa {MAX_SIZE//1024//1024}MB"
        )
    
    try:
        # Th·ª±c hi·ªán inference
        start_time = time.time()
        detections = infer(data)
        inference_time = (time.time() - start_time) * 1000
        
        # Th·ªëng k√™ k·∫øt qu·∫£
        class_counts = {}
        total_confidence = 0
        high_confidence_count = 0
        
        for det in detections:
            class_name = det["label"]
            class_counts[class_name] = class_counts.get(class_name, 0) + 1
            total_confidence += det["confidence"]
            if det["confidence"] > HIGH_CONFIDENCE_THRESHOLD:
                high_confidence_count += 1
        
        avg_confidence = total_confidence / len(detections) if detections else 0
        
        return {
            "success": True,
            "image_info": {
                "filename": file.filename,
                "size_bytes": len(data),
                "content_type": file.content_type
            },
            "inference": {
                "time_ms": round(inference_time, 2),
                "model": "YOLOv12n Balanced"
            },
            "results": {
                "total_objects": len(detections),
                "high_confidence_objects": high_confidence_count,
                "average_confidence": round(avg_confidence, 3),
                "class_distribution": class_counts,
                "objects": detections
            },
            "timestamp": time.time()
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500, 
            detail=f"L·ªói x·ª≠ l√Ω ·∫£nh: {str(e)}"
        )